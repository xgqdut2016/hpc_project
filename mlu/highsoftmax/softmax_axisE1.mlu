#include <bang.h>
#include <bang_device_functions.h>
#define EPS 1e-7
const int NRAM_MAX_SIZE = 1024 * 128;//后续树状求和必须保证NRAM_MAX_SIZE为2的幂次
const int maxNum = NRAM_MAX_SIZE/sizeof(float); //NRAM上最多存储maxNum个float元素
const int warpSize = 32;
//dimS至少要等于dimsize，且是最近的2的幂次方，同时由于后面需要规约，为此dimS至少是32
//下面这个kernel只适合dimsize < maxNum的情况
template<int dimS>
__mlu_entry__ void softmaxKernelAxis_e(float* destination, float* source, int othersize, int dimsize) {// axis = -1
  int multiple = maxNum / dimsize;
  int size = taskDim * multiple;
  int remainS = othersize % size;
  int taskRepeat = (othersize - remainS) / size;
  int remainT = remainS % taskDim;
  int stepEasy = (remainS - remainT) / taskDim;
  int stepHard = stepEasy + 1;
  int step = (taskId < remainT ? stepHard : stepEasy);
  //每个taskId处理othersize分配的量就是taskRepeat * multiple + step
  //整体来看，每个taskId处理的数据量就是(taskRepeat * multiple + step) * dimsize
  int startHard = taskId * (taskRepeat * multiple + stepHard);
  int startEasy = remainT * (taskRepeat * multiple + stepHard) + (taskId - remainT) * (taskRepeat * multiple + stepEasy);
  int indStart = (taskId < remainT ? startHard: startEasy);
  source = source + indStart * dimsize;
  destination = destination + indStart * dimsize;
  //printf("taskRepeat:%d, indstart:%d, %d\n", taskRepeat, indStart, indStart * dimsize);
  __nram__ float src[maxNum];

  __nram__ float tmp[dimS];
  __nram__ float destSum[dimS];//后面数值求和
  __nram__ float destSumFinal[warpSize];//将destSum规约到destFinal[0]
  __nram__ float srcMax[2];
  

  int tid;
  for(int s = 0; s < taskRepeat; s++){
    
    tid = s * multiple * dimsize;
    __memcpy(src, source + tid, multiple * dimsize * sizeof(float), GDRAM2NRAM);
    for(int j = 0; j < multiple; j++){
      __bang_write_zero(destSum, dimS);
      __bang_write_zero(destSumFinal, warpSize);
      __bang_write_value(tmp, dimS, -INFINITY);

      __memcpy(tmp, src + j * dimsize, dimsize * sizeof(float), NRAM2NRAM);
      __bang_argmax(srcMax, tmp, dimS);
      __bang_write_value(tmp, dimS, srcMax[0]);//必须重新初始化为srcMax[0]
      __memcpy(tmp, src + j * dimsize, dimsize * sizeof(float), NRAM2NRAM);//必须要重新读取
      __bang_sub_scalar(tmp, tmp, srcMax[0], dimS);
      __bang_active_exp_less_0(tmp, tmp, dimS);//这里我们认为负无穷-srcMax[0]非常小，所以后面dimS - dimsize部分认为是0
      __bang_add(destSum, destSum, tmp, dimS);
      
      int segNum = dimS / warpSize;//开始数值求和
      for(int strip = segNum/2; strip > 0; strip = strip / 2){
        for(int i = 0; i < strip ; i++){
          __bang_add(destSum + i * warpSize, destSum + i * warpSize, destSum + (i + strip) * warpSize, warpSize);
        } 
      }
      __bang_reduce_sum(destSumFinal, destSum, warpSize);//此时destSumFinal[0]保存的就是当前dimsize长度数据的数值和
      destSumFinal[0] = destSumFinal[0] - (dimS - dimsize);
      //__bang_printf("max:%.2f, sum:%.2f\n", srcMax[0], destSumFinal[0]);
      float globalSumInv = 1.0/destSumFinal[0];
      __bang_mul_scalar(tmp, tmp, globalSumInv, maxNum);
      //__memcpy(src + j * dimsize, tmp, dimsize * sizeof(float), NRAM2NRAM);
      __memcpy(destination + tid + j * dimsize, tmp, dimsize * sizeof(float), NRAM2GDRAM);
    }//必须马上写回GDRAM，如果先写回src，然后src写回GDRAM，可能出现src写回GDRAM没有结束就修改src数据的情况
    //__memcpy(destination + tid, src, multiple * dimsize * sizeof(float), NRAM2GDRAM);
  }
  
  for(int s = 0; s < step; s++){
    tid = taskRepeat * multiple * dimsize + s * dimsize;
    __bang_write_zero(destSum, dimS);
    __bang_write_zero(destSumFinal, warpSize);
    __bang_write_value(tmp, dimS, -INFINITY);
    __memcpy(tmp, source + tid, dimsize * sizeof(float), GDRAM2NRAM);
    
    __bang_argmax(srcMax, tmp, dimS);
    __bang_write_value(tmp, dimS, srcMax[0]);
    __memcpy(tmp, source + tid, dimsize * sizeof(float), GDRAM2NRAM);
    __bang_sub_scalar(tmp, tmp, srcMax[0], dimS);
    
    __bang_active_exp_less_0(tmp, tmp, dimS);//后面dimS - dimsize部分是1
    __bang_add(destSum, destSum, tmp, dimS);
    
    int segNum = dimS / warpSize;//开始数值求和
    for(int strip = segNum/2; strip > 0; strip = strip / 2){
      for(int i = 0; i < strip ; i++){
        __bang_add(destSum + i * warpSize, destSum + i * warpSize, destSum + (i + strip) * warpSize, warpSize);
      }
    }
    __bang_reduce_sum(destSumFinal, destSum, warpSize);//此时destSumFinal[0]保存的就是当前dimsize长度数据的数值和
    destSumFinal[0] = destSumFinal[0] - (dimS - dimsize);
    //__bang_printf(":%.2f,max:%.2f, sum:%.2f, final:%.2f\n",tmp[1], srcMax[0], destSum[1], destSumFinal[0]);
    float globalSumInv = 1.0/destSumFinal[0];
    __bang_mul_scalar(tmp, tmp, globalSumInv, maxNum);
    __memcpy(destination + tid, tmp, dimsize * sizeof(float), NRAM2GDRAM);
  }
  //__bang_printf("max:%.2f, sum:%.2f\n", srcMax[0], destSumFinal[0]);
  
}


int main(void)
{
  //int shape[4] = {1024,128,32,32};
  //int shape[4] = {1024,64,32,32};
  int shape[4] = {1024,32,32,32};
  //int shape[4] = {2, 3, 2, 2};
  int axis = 3;
  int stride = 1;
  int dimsize = shape[axis];
  int num = 1;
  int othersize = 1;
  for(int s = 3; s >= 0; s--){
    num *= shape[s];
    if(s > axis){
      stride *= shape[s];
    }
    if(s != axis){
      othersize *= shape[s];
    }
  }
  
  printf("axis:%d, dimsize:%d, stride:%d, othersize:%d, num:%d\n", axis, dimsize, stride, othersize, num);
  cnrtQueue_t queue;
  CNRT_CHECK(cnrtSetDevice(0));
  CNRT_CHECK(cnrtQueueCreate(&queue));

  cnrtDim3_t dim = {4, 1, 1};
  int taskNum = dim.x * dim.y * dim.z;
  cnrtFunctionType_t ktype = CNRT_FUNC_TYPE_UNION1;

  cnrtNotifier_t start, end;
  CNRT_CHECK(cnrtNotifierCreate(&start));
  CNRT_CHECK(cnrtNotifierCreate(&end));

  float* host_destination = (float*)malloc(num * sizeof(float));
  float* host_src = (float*)malloc(num * sizeof(float));
  

  for (int i = 0; i < num; i++) {
    host_src[i] = i%4;
    //host_src[i] = i;
  }

  float* mlu_destination;
  float* mlu_src;
  
  CNRT_CHECK(cnrtMalloc((void**)&mlu_destination, num * sizeof(float)));
  CNRT_CHECK(cnrtMalloc((void**)&mlu_src, num * sizeof(float)));
  

  CNRT_CHECK(cnrtMemcpy(mlu_src, host_src, num * sizeof(float), cnrtMemcpyHostToDev));
  
  //----------------------------
  CNRT_CHECK(cnrtPlaceNotifier(start, queue));
  softmaxKernelAxis_e<32><<<dim, ktype, queue>>>(mlu_destination, mlu_src, othersize, dimsize);
  CNRT_CHECK(cnrtPlaceNotifier(end, queue));
  cnrtQueueSync(queue);

  //---------------------------
  CNRT_CHECK(cnrtMemcpy(host_destination, mlu_destination, num * sizeof(float), cnrtMemcpyDevToHost));
  for(int i = 0; i < 24; i++){
    printf("softmax[%d]:%.6e,origin:%.6f\n", i, host_destination[i], host_src[i]);
  }
  float timeTotal;
  CNRT_CHECK(cnrtNotifierDuration(start, end, &timeTotal));
  printf("Total Time: %.3f ms\n", timeTotal / 1000.0);

  CNRT_CHECK(cnrtQueueDestroy(queue));

  cnrtFree(mlu_destination);
  cnrtFree(mlu_src);
  
  
  free(host_destination);
  free(host_src);
  

  return 0;
}
                           

